#聚類任務
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time

from sklearn.datasets import load_wine
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, adjusted_rand_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.model_selection import ParameterGrid

# Load and scale the dataset
wine = load_wine()
X, y = wine.data, wine.target
feature_names = wine.feature_names

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Visualize raw data using PCA (original labels)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(6, 4))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='Set1', alpha=0.6)
plt.title('Original Wine Classes (PCA Projection)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.tight_layout()
plt.savefig('/content/raw_data_pca_visualization.png')
plt.close()

# Define clustering models with hyperparameter grid
model_configs = {
    'K-Means': {
        'model': KMeans,
        'params': {'n_clusters': [2, 3, 4], 'random_state': [42]}
    },
    'Agglomerative Clustering': {
        'model': AgglomerativeClustering,
        'params': {'n_clusters': [2, 3, 4], 'linkage': ['ward', 'average', 'complete']}
    },
    'DBSCAN': {
        'model': DBSCAN,
        'params': {'eps': [1.5, 2.0, 2.5], 'min_samples': [3, 5, 10]}
    }
}

# Evaluate models
results = []
for name, config in model_configs.items():
    for params in ParameterGrid(config['params']):
        model = config['model'](**params)
        start_time = time.time()
        labels = model.fit_predict(X_scaled)
        end_time = time.time()

        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1) if -1 in labels else 0
        silhouette = silhouette_score(X_scaled, labels) if n_clusters > 1 else np.nan
        ari = adjusted_rand_score(y, labels)
        db_score = davies_bouldin_score(X_scaled, labels) if n_clusters > 1 else np.nan
        ch_score = calinski_harabasz_score(X_scaled, labels) if n_clusters > 1 else np.nan

        results.append([
            name, str(params), silhouette, ari, db_score, ch_score,
            n_clusters, n_noise, end_time - start_time
        ])

        # Save PCA clustering plot
        plt.figure(figsize=(6, 4))
        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.6)
        plt.title(f'{name} ({params})')
        plt.xlabel('Principal Component 1')
        plt.ylabel('Principal Component 2')
        plt.tight_layout()
        fname = f"{name.replace(' ', '_')}_{str(params).replace(':', '').replace(',', '').replace(' ', '_')}_pca.png"
        plt.savefig(f"/content/{fname}")
        plt.close()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# 計算不同 K 值的 SSE（Inertia）
sse = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)  # 原始13維資料或你也可以改用PCA降維後資料
    sse.append(kmeans.inertia_)

# 畫出 Elbow 曲線
plt.figure(figsize=(6, 4))
plt.plot(K_range, sse, 'bo-')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('SSE (Inertia)')
plt.title('Elbow Method for Optimal K')
plt.xticks(K_range)
plt.grid(True)
plt.tight_layout()
plt.savefig("kmeans_elbow_curve.png")
plt.show()

# Create results dataframe
results_df = pd.DataFrame(results, columns=[
    'Model', 'Parameters', 'Silhouette Score', 'Adjusted Rand Index',
    'Davies-Bouldin Index', 'Calinski-Harabasz Index', 'Number of Clusters',
    'Noise Points', 'Runtime (s)'
])

results_df.sort_values(by='Adjusted Rand Index', ascending=False, inplace=True)
results_df.reset_index(drop=True, inplace=True)
results_df
